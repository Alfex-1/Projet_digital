{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ce notebook rassemble les méthodes statistiques employées pour la sélection de variables pour le modèle de régression logistique ordinale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mord import LogisticAT\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mord import LogisticAT\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = pd.read_csv(\"../../data/train.csv\")\n",
    "#df_test = pd.read_csv(\"../../data/test.csv\")\n",
    "df_train = pd.read_csv(\"C://Cours M2 S1//Conférences, SAS, VBA, Gestion de projet//Gestion de projet digital//train.csv\")\n",
    "df_test = pd.read_csv(\"C://Cours M2 S1//Conférences, SAS, VBA, Gestion de projet//Gestion de projet digital//test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_sets(train_data, test_data):\n",
    "    \"\"\"Fonction qui récupère les dataframes train et test et retourne les vecteurs utiles pour la phase\n",
    "    d'entrainement et test.\n",
    "\n",
    "    Args:\n",
    "        train_data (DataFrame): données d'entrainement pré-traitées\n",
    "        test_data (DataFrame): données de test pré-traitées\n",
    "\n",
    "    Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train = train_data.drop(labels=[\"score\"], axis=1).values\n",
    "    y_train = train_data[\"score\"].values\n",
    "\n",
    "    X_test = test_data.drop(labels=[\"score\"], axis=1).values\n",
    "    y_test = test_data[\"score\"].values\n",
    "    \n",
    "    ## On normalise les données\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_sets(train_data=df_train, test_data=df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le modèle de régression logistique ordinale initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On regarde les coefficients et l'erreur de classification du modèle global initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On entraîne le modèle initial\n",
    "\n",
    "def model_training(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Fonction pour entrainer le modèle de régression logistique ordinale\n",
    "\n",
    "    Args:\n",
    "        X_train (Array): \n",
    "        X_test (Array): \n",
    "        y_train (Array): \n",
    "        y_test (Array): \n",
    "    \"\"\"\n",
    "\n",
    "    # Maintenant, on utilise les variables sélectionnées pour la régression logistique ordinale avec mord\n",
    "    model_ordinal = LogisticAT(alpha=0)  # On utilise alpha=0 pour ne pas faire de régularisation\n",
    "\n",
    "    # On entraîne le modèle sur les données d'entraînement avec les variables sélectionnées\n",
    "    model_ordinal.fit(X_train, y_train)\n",
    "\n",
    "    # prédictions sur les données de test avec les variables sélectionnées\n",
    "    predictions_ordinal = model_ordinal.predict(X_test)\n",
    "\n",
    "    # Calcul de l'exactitude du modèle de régression logistique ordinale\n",
    "    accuracy_ordinal = accuracy_score(y_test, predictions_ordinal)\n",
    "    \n",
    "    print(\"Coefficients du modèle de régression logistique :\")\n",
    "    print(model_ordinal.coef_)\n",
    "   \n",
    "    print(\"Précision du modèle sur les données de test : {:.2f}%\".format(accuracy_ordinal * 100))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients du modèle de régression logistique :\n",
      "[-8.69431442e-01 -8.68057631e-01 -2.52819023e+00 -6.25124049e-01\n",
      " -1.17822450e+00 -2.90415631e-02 -2.14414883e+00  1.16671329e+00\n",
      "  3.17895520e-01 -1.00617077e+02  9.87501392e+01 -3.81371512e-02\n",
      " -7.46649800e-02 -3.59015466e-01  1.16666886e+00]\n",
      "Précision du modèle sur les données de test : 73.92%\n"
     ]
    }
   ],
   "source": [
    "#On l'applique et on montre ses coefficients et son erreur de classification\n",
    "model_training(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthodes de sélection de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1-* Sélection par régression Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegressionCV est utilisé pour effectuer une régression logistique avec une validation croisée pour sélectionner le meilleur paramètre de régularisation (C) parmi une liste de valeurs candidates ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_lasso(train_data, test_data, max_iter = 1000, seuil=0.2):\n",
    "    \"\"\"Fonction pour faire la sélection de variables en se basant sur un modèle Lasso.\n",
    "\n",
    "    Args:\n",
    "        train_data (Dataframe): données préparées pour le train\n",
    "        test_data (DataFrame): données préparées pour le test\n",
    "        max_iter (int, optional): Nombre d'itérations pour obtenir la convergence, par défaut 1000\n",
    "        seuil (float, optional): Valeur seuil en valeur absolur au dessus de laquelle on sélectionn les variables. Par défaut 0.2.\n",
    "\n",
    "    Returns:\n",
    "        tuple: variables_lasso (Liste) noms des variables choisies, X_train_final (Array), X_test_final (Array), y_train(Array), y_test (Array)\n",
    "        vecteurs des variables choisies\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = get_train_test_sets(train_data=train_data, test_data=test_data)\n",
    "    \n",
    "\n",
    "\n",
    "    # modèle de régression logistique avec pénalité L1 (LASSO)\n",
    "    model_lasso = LogisticRegression(penalty='l1', solver='saga', max_iter=max_iter, random_state=None)\n",
    "\n",
    "    # SelectFromModel pour récupérer les paramètres du modèle\n",
    "    sfm = SelectFromModel(estimator=model_lasso)\n",
    "    sfm.fit(X_train, y_train)\n",
    "\n",
    "    # indices des variables sélectionnées\n",
    "    indices_lasso = sfm.get_support(indices=True)\n",
    "    \n",
    "    # noms des variables correspondantes dans les données\n",
    "    variables = [train_data.columns[i] for i in indices_lasso]\n",
    "    \n",
    "    # coefficients des variables sélectionnées\n",
    "    coefficients_lasso = sfm.estimator_.coef_.flatten()\n",
    "\n",
    "    # noms et coefficients des variables\n",
    "    print(\"#########################################################\")\n",
    "    print()\n",
    "    print(\"Coefficients obtenus après Lasso:\")\n",
    "    for variable, coefficient in zip(variables, coefficients_lasso):\n",
    "        print(f\"{variable}: {coefficient}\")\n",
    "\n",
    "    # On pénalise en ne prenant que les coeffs différents au dessus du seuil passé en argument\n",
    "    indices_choisis = [i for i, coef in zip(indices_lasso, coefficients_lasso) if abs(coef) > seuil]\n",
    "\n",
    "    # variables correspondantes aux indices choisies\n",
    "    variables_lasso = [train_data.columns[i] for i in indices_choisis]\n",
    "\n",
    "    print(\"##########################################################\")\n",
    "    print()\n",
    "    print(\"Variables sélectionnées après Lasso\")\n",
    "    print()\n",
    "    for variable in variables_lasso:\n",
    "        print(f\"{variable}\")\n",
    "\n",
    "\n",
    "    # on utilise les indices filtrés pour obtenir les données des variables optimales sélectionnées\n",
    "    X_train_final = X_train[:, indices_choisis]\n",
    "    X_test_final = X_test[:, indices_choisis]\n",
    "    \n",
    "    return variables_lasso, X_train_final, X_test_final, y_train, y_test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On effectue la sélection de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################\n",
      "\n",
      "Coefficients obtenus après Lasso:\n",
      "energy_100g: 1.480942639366321\n",
      "fat_100g: 1.1784743246135954\n",
      "saturated-fat_100g: 5.0696470696560825\n",
      "trans-fat_100g: 1.5001631855858724\n",
      "cholesterol_100g: 1.737175704050646\n",
      "carbohydrates_100g: 0.6348745186366846\n",
      "sugars_100g: 2.9465936128379755\n",
      "fiber_100g: -1.958085840128126\n",
      "proteins_100g: 0.0\n",
      "salt_100g: 1.2795379031110787\n",
      "sodium_100g: 1.2801899356246484\n",
      "vitamin-a_100g: 0.8938106508001012\n",
      "vitamin-c_100g: 0.5018054579698419\n",
      "calcium_100g: 0.0\n",
      "iron_100g: -7.487952521369647\n",
      "##########################################################\n",
      "\n",
      "Variables sélectionnées après Lasso\n",
      "\n",
      "energy_100g\n",
      "fat_100g\n",
      "saturated-fat_100g\n",
      "trans-fat_100g\n",
      "cholesterol_100g\n",
      "carbohydrates_100g\n",
      "sugars_100g\n",
      "fiber_100g\n",
      "salt_100g\n",
      "sodium_100g\n",
      "vitamin-a_100g\n",
      "vitamin-c_100g\n",
      "iron_100g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "_, X_train_lasso, X_test_lasso, y_train, y_test = selection_lasso(train_data=df_train, test_data=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(X_train, X_test, y_train, y_test, selection_method = 'lasso'):\n",
    "    \"\"\"Fonction pour entrainer le modèle de régression logistique ordinale\n",
    "\n",
    "    Args:\n",
    "        X_train (Array): \n",
    "        X_test (Array): \n",
    "        y_train (Array): \n",
    "        y_test (Array): \n",
    "    \"\"\"\n",
    "\n",
    "    # Maintenant, on utilise les variables sélectionnées pour la régression logistique ordinale avec mord\n",
    "    model_ordinal = LogisticAT(alpha=0)  # On utilise alpha=0 pour ne pas faire de régularisation\n",
    "\n",
    "    # On entraîne le modèle sur les données d'entraînement avec les variables sélectionnées\n",
    "    model_ordinal.fit(X_train, y_train)\n",
    "\n",
    "    # prédictions sur les données de test avec les variables sélectionnées\n",
    "    predictions_ordinal = model_ordinal.predict(X_test)\n",
    "\n",
    "    # Calcul de l'exactitude du modèle de régression logistique ordinale\n",
    "    accuracy_ordinal = accuracy_score(y_test, predictions_ordinal)\n",
    "\n",
    "    print()\n",
    "    print(\"Précision du modèle : {:.2f}%\".format(accuracy_ordinal * 100))\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraine le modèle sur les variables sélectionnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Précision du modèle : 72.58%\n"
     ]
    }
   ],
   "source": [
    "model_training(X_train=X_train_lasso, X_test=X_test_lasso, y_train=y_train, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *2-* Sélection par Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_selection(train_data, test_data, expected_nb_var, nb_estimator =100):\n",
    "    \"\"\"Fonction pour faire de la sélection de variables en fonction de l'importance des variables obtenues en\n",
    "    utilisant une modèle de RandomForestClassifier\n",
    "\n",
    "    Args:\n",
    "        train_data (DataFrame): dataframe d'entrainement\n",
    "        test_data (DataFrame): dataframe de test\n",
    "        expected_nb_var (int): nombre de variables qu'on souhaite obtenir\n",
    "        nb_estimator (int) : nombres d'arbres dans la forêt\n",
    "    \"\"\"\n",
    "\n",
    "    # on obtient les matrices (train & test) à passer dans le modèle\n",
    "    X_train, X_test, y_train, y_test = get_train_test_sets(train_data=train_data, test_data=test_data)\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "\n",
    "    # modèle Random Forest avec bootstrap\n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators=nb_estimator, random_state=None, bootstrap=True, criterion='gini')\n",
    "    \"\"\" n-estimators : On aura donc 100 arbres qui seront tous légèrement différents\n",
    "    \n",
    "        bootstrap : construire de nouveaux échantillons par tirage aléatoire avec remise\n",
    "        \n",
    "        criterion = 'gini' : L'indice de Gini mesure l'impureté d'un nœud (une division) \n",
    "        dans un arbre de décision. Plus précisément, il évalue à quel point un nœud est mélangé en termes de classes de la cible\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Entraînement du modèle sur les données\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Importance des variables à partir du modèle\n",
    "    feature_importances = rf_model.feature_importances_\n",
    "\n",
    "    # Sélection des indices des variables importantes (les nvar premières)\n",
    "    important_indices = feature_importances.argsort()[-expected_nb_var:][::-1]\n",
    "    \n",
    "    variables_rf = [train_data.columns[i] for i in important_indices]\n",
    "    \n",
    "    print(\"##########################################################\")\n",
    "    print()\n",
    "    print(\"Variables sélectionnées après Random Forest\")\n",
    "    print()\n",
    "    for variable in variables_rf:\n",
    "        print(f\"{variable}\")\n",
    "\n",
    "    # Matrices d'entrainement et de test pour entrainer le modèle logistique\n",
    "    X_train_rf = X_train[:, important_indices]\n",
    "    X_test_rf = X_test[:, important_indices]\n",
    "    \n",
    "    return variables_rf, X_train_rf, X_test_rf, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################\n",
      "\n",
      "Variables sélectionnées après Random Forest\n",
      "\n",
      "nutrition-score-fr_100g\n",
      "trans-fat_100g\n",
      "saturated-fat_100g\n",
      "calcium_100g\n",
      "fat_100g\n",
      "cholesterol_100g\n",
      "energy_100g\n",
      "code\n",
      "sugars_100g\n",
      "vitamin-c_100g\n"
     ]
    }
   ],
   "source": [
    "_, X_train_rf, X_test_rf, y_train, y_test = random_forest_selection(train_data=df_train, test_data=df_test, nvar=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
